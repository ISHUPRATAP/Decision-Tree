{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1\n",
        "A Decision Tree is a supervised machine learning algorithm widely used for classification tasks. It works by breaking down a dataset into smaller subsets based on feature values, forming a tree-like structure. The process starts at the root node (entire dataset), then splits data at internal nodes using criteria like Information Gain or Gini Index to choose the best feature. Each branch represents a decision outcome, and the process continues until reaching leaf nodes, which represent the final class labels. For classification, a new sample is classified by tracing its feature values from the root down to a leaf. Decision Trees are popular because they are simple, interpretable, and mimic human decision-making."
      ],
      "metadata": {
        "id": "92br5_8_9fm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2\n",
        "Gini Impurity:\n",
        "\n",
        "1) Gini impurity measures how often a randomly chosen sample would be incorrectly classified if it was randomly labeled according to the class distribution in a node.\n",
        "\n",
        "2)Value ranges from 0 (pure node: only one class) to 0.5 (maximum impurity in binary classification).\n",
        "\n",
        "3)The lower the Gini value, the purer the node.\n",
        "\n",
        "Entropy:\n",
        "\n",
        "Entropy is an information theory measure of uncertainty in a dataset.\n",
        "\n",
        "\n",
        "Value is 0 when the node is pure (all samples same class) and higher when classes are mixed.\n",
        "\n",
        "Entropy is used to calculate Information Gain, which measures how much uncertainty is reduced after a split.\n",
        "\n",
        "Impact on Splits in Decision Trees:\n",
        "\n",
        "1)Both Gini Impurity and Entropy guide the tree in selecting the best feature and split point.\n",
        "\n",
        "2)The algorithm evaluates all possible splits and chooses the one that gives the largest reduction in impurity (highest purity gain).\n",
        "\n",
        "3)Practically, Gini tends to be slightly faster to compute, while Entropy is more theoretically grounded in information theory, but both usually produce very similar trees."
      ],
      "metadata": {
        "id": "-cg3hvsw-IR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 3\n",
        "\n",
        "Pre-Pruning (Early Stopping):\n",
        "\n",
        "Pre-pruning stops the tree growth early by applying conditions such as maximum depth, minimum samples per split, or minimum information gain.\n",
        "\n",
        "It prevents the tree from becoming too complex.\n",
        "\n",
        "Advantage: Saves time and avoids overfitting by keeping the tree simpler.\n",
        "\n",
        "Post-Pruning (Pruning After Full Growth):\n",
        "\n",
        "Post-pruning allows the tree to grow fully and then removes branches that add little predictive power, often using validation data.\n",
        "\n",
        "It reduces complexity after observing the complete structure.\n",
        "\n",
        "Advantage: Produces a more accurate and generalized model since it prunes only unnecessary branches."
      ],
      "metadata": {
        "id": "0b4cSwvd_PkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 4\n",
        "\n",
        "Information Gain:\n",
        "Information Gain is a measure used in Decision Trees to decide the best feature for splitting the data. It is based on Entropy and shows how much uncertainty (impurity) is reduced after a split.\n",
        "\n",
        "Importance:\n",
        "\n",
        "A higher Information Gain means the split gives a clearer separation of classes.\n",
        "\n",
        "The Decision Tree selects the feature with the maximum Information Gain at each step.\n",
        "\n",
        "This ensures the tree becomes more accurate and efficient, reducing impurity quickly."
      ],
      "metadata": {
        "id": "8PRyCp2f_d5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 5\n",
        "\n",
        "**Real-World Applications of Decision Trees:**\n",
        "\n",
        "* **Finance:** Credit scoring, loan approval, fraud detection.\n",
        "* **Healthcare:** Disease diagnosis, treatment planning.\n",
        "* **Marketing:** Customer segmentation, predicting churn, product recommendations.\n",
        "* **Operations:** Risk analysis, decision support systems.\n",
        "* **Education:** Predicting student performance or dropout risk.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Simple to understand and interpret (like human decision-making).\n",
        "2. Can handle both numerical and categorical data.\n",
        "3. Requires little data preprocessing (no need for scaling/normalization).\n",
        "4. Works well for feature selection by identifying important variables.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. Prone to **overfitting** if not pruned.\n",
        "2. Can be **unstable** (small data changes may create a different tree).\n",
        "3. Biased toward features with more levels.\n",
        "4. Less accurate compared to ensemble methods like Random Forests.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBr8Hc_S_uSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4xY_PLVaACKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 6\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC5px65yA_E9",
        "outputId": "c2c28857-845b-4292-b233-a90190ed30d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 7\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree with max_depth = 3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree with max_depth=3 Accuracy:\", accuracy_limited)\n",
        "print(\"Fully-grown Decision Tree Accuracy:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQFRSRGBVwm",
        "outputId": "70f0f569-b240-45ef-dddb-3360cb840206"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with max_depth=3 Accuracy: 1.0\n",
            "Fully-grown Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 8\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, reg.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gofp2DJMBaej",
        "outputId": "486b4b88-9ae8-4ae6-bdf8-c26fe6c21df4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 9\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and train the model with them\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data and calculate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWcjbG5JBmqO",
        "outputId": "3d114f29-3329-461a-b1b8-199ae90cb297"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 10\n",
        "\n",
        "**Step-by-Step Process for Building a Decision Tree Model in Healthcare**\n",
        "\n",
        "1. **Handle Missing Values:**\n",
        "\n",
        "   * **Identify missing data** in the dataset.\n",
        "   * For **numerical features**, impute missing values using the **mean or median**.\n",
        "   * For **categorical features**, use the **mode** or a special category like “Unknown.”\n",
        "   * Optionally, use more advanced techniques like **KNN imputation** for better accuracy.\n",
        "\n",
        "2. **Encode Categorical Features:**\n",
        "\n",
        "   * Convert categorical variables into numerical format because Decision Trees can handle **ordinal or one-hot encoded variables**.\n",
        "   * For **nominal categories**, use **One-Hot Encoding**.\n",
        "   * For **ordinal categories**, map them to **integer values** representing their order.\n",
        "\n",
        "3. **Train a Decision Tree Model:**\n",
        "\n",
        "   * Split the dataset into **training and testing sets**.\n",
        "   * Initialize a **Decision Tree Classifier**, choosing a criterion like **Gini** or **Entropy**.\n",
        "   * Fit the model on the **training data**.\n",
        "\n",
        "4. **Tune Hyperparameters:**\n",
        "\n",
        "   * Use **GridSearchCV** or **RandomizedSearchCV** to find the best parameters:\n",
        "\n",
        "     * `max_depth` (controls tree depth)\n",
        "     * `min_samples_split` (minimum samples to split a node)\n",
        "     * `min_samples_leaf` (minimum samples in a leaf)\n",
        "     * `criterion` (Gini or Entropy)\n",
        "   * Helps reduce **overfitting** and improves generalization.\n",
        "\n",
        "5. **Evaluate Model Performance:**\n",
        "\n",
        "   * Use metrics suitable for classification:\n",
        "\n",
        "     * **Accuracy**: Overall correctness.\n",
        "     * **Precision & Recall**: Especially important in healthcare to minimize false negatives/positives.\n",
        "     * **F1-Score**: Balance between precision and recall.\n",
        "     * **Confusion Matrix**: To understand types of errors.\n",
        "   * Optionally, use **cross-validation** to ensure stability.\n",
        "\n",
        "6. **Business Value in Real-World Healthcare:**\n",
        "\n",
        "   * **Early detection of diseases** helps in timely treatment and improves patient outcomes.\n",
        "   * **Resource optimization:** Hospitals can allocate resources efficiently to high-risk patients.\n",
        "   * **Decision support for doctors:** Provides data-driven insights to complement medical expertise.\n",
        "   * **Reducing healthcare costs** by focusing preventive care on likely patients.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tDjz8UwsCEjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bV_LxPVvCNdx"
      }
    }
  ]
}